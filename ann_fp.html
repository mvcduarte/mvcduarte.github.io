<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Marcus Duarte webpage</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <!-- MathJax -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="posts.html">Posts</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="http://www.astro.iag.usp.br/~mvcduarte/">Astronomy</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="cv/cv.pdf">CV</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="contact.html">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-image: url('img/rockets_nasa.jpg')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="site-heading">
              <h1>Welcome to my webpage!</h1>
              <span class="subheading">Machine Learning and Statistical Inferences</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">

            <h4 class="subheading">Artificial Neural Networks - Part 1: Foward Propagation</h4>

            <p>
            <p, style="text-align:justify"> 
            In this serie of Neural Network posts, I will demostrate how to implement the Artificial Neural Network (<a href='https://en.wikipedia.org/wiki/Artificial_neural_network'>ANN</a>) algorithm from scratch. 
            </p>

            <p, style="text-align:justify">
            Nowadays, the expressions "Neural Network" and "Deep Learning" are quite fashion in the Big Data and Data Science tutorials and blogs. This algorithm was inspired by biological brain structures (neurons) and its complexity increased a lot since the 70s and 80s. Today, modern computers or clusters allow us to construct complex ANN to classify and predict values in diverse problems in many companies and Universities.         
            </p>
            
            <p, style="text-align:justify">
            The ANN is composed by a collection of interconnected nodes (artificial neurons) distributed in layers. The connections are responsible to transmit the information from one neuron to another. Once the information arrives at a certain neuron or node, it is firstly processed and then transmited foward. Figure 1 shows the distribution of an ANN, composed by an Input Layer, one Hidden Layer (with \(n_{hidden}\) nodes) and Output Layer. The arrows represent the connections between the nodes. In this context, the "Deep Learning" is commonly referred as a Neural Network with several hidden layers. Initially, the ANN is not able to properly to classify or predict anything. Since it is not trained, the weights and bias terms from all layers are not adjusted to carry out this task. To do so, we use namely two samples of data. The first one is the training sample, which is used to obviouly train the ANN and the second one is the test sample. The former one evaluates the performance of the trained algorithm after the training procedure. As a rule of thumb, the fraction between the training and test samples is around 70%/30% or 60%/40%. In this post, we explain the architecture of the ANN and how the training dataset passes through the Neural Network. The training procedure also needs the Back Propagation, which will be described in the next post.        
            </p>

            <figure>
            <img src="graphs/ann.png" class="center">
            <figcaption><b>Figure 1</b>: The Artificial Neural Network composed by the Input Layer (2 nodes), the Hidden Layer (5 nodes) and Output Layer (2 nodes). </figcaption>
            </figure>

            <p, style="text-align:justify">
            Mathematically, the ANN is a serie of matrix multiplication, including weights, input data and bias factors that enter at the left side and pass through the net up to the output layer at the right (Figure 1). Following the ANN showed above, the input and output layers contain 2 nodes. The training data is represented here by the matrix \(X\) with dimensions (\(n_{obj}, n_{feature})\). Between two consecutive layers, the data is multiplied by a weight factor and a bias factor is summed. It means that between the Input and Hidden Layers, we have the following matrix multiplication,   
            </p>
            $$ z_1 = X * W_1 + b_1$$

            being the dimensions of the weight and bias are equal to (\(n_{feature}, n_{hidden})\) and (\(n_{hidden}, 1)\), respectively. Note that the bias term \(b_1\) is just an additive term and the dimension of \(a_2\) is consequently (\(n_{obj}, n_{hidden})\). The powerful skill of ANNs is represented by their capacity of complex and non-linearity modelling. Just considering the matrix multiplication above is not enough to reach such complexity. The next step is to pass our multiplication matrix result through an activation function, which decides whether it should be considered (or not) as a good or bad result for the layer, given the necessary complexity to fit the model to the data. There are many activation functions around but the main and most famous activation function is the <a href='https://en.wikipedia.org/wiki/Sigmoid_function'>sigmoid</a>. Figure 2 shows the profile of the sigmoid function., being 

            $$ sigmoid(x) = \frac{1}{1 + e^{-x}}$$

            <figure>
            <img src="graphs/sigmoid.png" class="center">
            <figcaption><b>Figure 2</b>: The sigmoid function.</figcaption>
            </figure>

            The values of the sigmoid function spam between 0 and 1. The activated values of the Hidden Layer is then defined as \(a_2 = sigmoid(z_1)\). Other examples of activation functions are <a href='https://en.wikipedia.org/wiki/Softmax_function'>softmax</a>, rectified linear unit (<a href='https://en.wikipedia.org/wiki/Rectifier_(neural_networks)'>ReLU</a>) and \(tanh(x)\).
            </p>

            <p>
            In the Python syntax, the activation function and the matrix multiplication between the Input and Hidden Layers can be written as,  
            </p> 

 
            <code style="color:black">
              import numpy as np
            </code>
            <code style="color:green"><p>
            def </code><code style="color:red">sigmoid</code><code style="color:black">(x):</br>
                &emsp;&emsp;"""</br>
                &emsp;&emsp; Sigmoid activation function</br>
                &emsp;&emsp;"""</br>
                &emsp;&emsp;if deriv == False:</br>
                &emsp;&emsp;&emsp;&emsp;return 1.0/(1 + np.exp(-x))</br>
                &emsp;&emsp;else:</br>
                &emsp;&emsp;&emsp;&emsp;return x * (1.0 - x) </br>
            </br>
            # Input -> Hidden Layer
            </br>

            z1 = X.dot(W1) + b1 </br>
            a2 = sigmoid(z1, deriv = False) </br>
            </br>
            </code>

            <p>
            Similar procedure between the Input and Hidden Layers is done now between the Hidden Layer and the Output Layer. Note that now we use the output of the hidden layer \(a_2\) as the new input. Thus, 
            </p>

            $$a_3 = softmax(a_2 * W_2 + b_2)$$
            <p>
            Between the Hidden Layer and the Output Layer, we just use a different activation function. Here we use the softmax, which is a good choice for probability classifications. It can be written as, 

            $$ softmax(x) = \frac{e^x}{\sum_i e^{x_i}}$$

            In Python syntax, it is defined as the  the <code style="color:red">softmax</code> routine, 
            <p>

            <code style="color:green"><p>
            def </code><code style="color:red">softmax</code><code style="color:black">(x):</br>
                &emsp;&emsp;"""</br>
                &emsp;&emsp; Softmax activation function</br>
                &emsp;&emsp;"""</br>
                &emsp;&emsp;exp_x = np.exp(x)</br>
                &emsp;&emsp;return exp_x / exp_x.sum(axis=1, keepdims=True)</br>
            </br>
            # Hidden -> Output Layer
            </br>
              z2 = a2.dot(W2) + b2</br> 
              a3 = softmax(z2)</br>
            </code>
            <p>
            Remember that the dimension of \(a_2\) is (\(n_{obj}, n_{hidden})\) and the weights and bias between the Hidden and Ouput Layers are represented as \(W_2\) and \(b_2\). For the matrix multiplication, their matricial dimensions are (\(n_{hidden}, n_{output})\) and (\(n_{output}, 1)\), respectively. The value \(n_{output}\) represents the number of values atributed to each object in the Output Layer. This can be considered as classes or regression values. For instance, in case of bimodal classification, the output array length is 2 for each object in the training sample. In addition, it can be also considered as proabilities of this element to belong to classes, e.g., \([0.25, 0.75]\). 
            </p>
            <p>
              How many nodes should my ANN have? And how about the number of hidden layers? It really depends on how complex your data is. If you increase the number of nodes in the Hidden Layer, your model will be able to fit more complex and non-linear data. However, it will be more affected by <a href='https://en.wikipedia.org/wiki/Overfitting'>overfitting</a>, which is namely when your model only works on your training sample and fails on the test or any other sample. That's why it is so important to always evaluate the performance of your code with an independent (but representative) sample.  
            </p>
            <p>
             Increasing the number of hidden layers will consequently make your model more complex but you should think carefully before increasing the complexity of the ANN. Again, the overfitting can kill all your performance showing disapointing results on other samples. The increasing complexity also increases the computational time, particularly if your training sample is large enough. It can increase a lot your computational time, being most of time inpractical using an ordinary computer.  
            </p>

            <p>
              It is important to know how ANNs work but all this process above is already optimized by several packages in Python and R. Some examples are represented by <a href='https://keras.io/'>Keras</a>, <a href='https://www.tensorflow.org/'>Tensorflow</a> and <a href='http://deeplearning.net/software/theano/'> Theano</a>. Some nice explanations of how ANN works are also in Data Science and Machine Learning blogs, then check the references below. If you want the full code of this post for didatic purposes, please check my <a href='https://github.com/mvcduarte/Neural-Network'>GitHub - Neural Network from Scratch</a> using the <a href='http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html'>make_moons</a> dataset. This post was just half of the story, the next part is the <a href='ann_bp.html'>Neural Networks - Part 2: BackPropagation </a>. If you have comments/suggestions about this post, please send me an <a href="mailto:mvcduarte@gmail.com?Subject=blog_ML" target="_top">e-mail</a>.
            </p>
            <!--
            <figure>
            <img class="img-fluid" src="graphs/animation_ann.gif" alt="">
            <figcaption>Figure 2: The training sample in the feature space. Points in different colours represent distinct classes and the shaded areas shows the probability regions of classes. </figcaption>
            </figure>
            -->

            <p><b>Some references:</b></p>
              <p>
            <ul>
            <li><a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">Wild ML</a>
            </li>
            <li> 
              <a href="https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/">The Clever Machine's Blog"</a>
            </li>
            <li>
              <a href="https://beckernick.github.io/neural-network-scratch/">Nick Becker's blog</a>
            </li>

            <li>
              <a href="http://www.cristiandima.com/neural-networks-from-scratch-in-python/">Cristian Dima's Blog</a>
            </li>
            </ul>
            <!--
            $$ {J(\theta) =\frac{1}{2m} [\sum^m_{i=1}(h_\theta(x^{(i)}) - y^{(i)})2 + \lambda\sum^n_{j=1}\theta^2_j} $$
            -->

          </div>
        </div>
      </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://www.facebook.com/marcus.duarte.92">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/mvcduarte">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Your Website 2018</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
