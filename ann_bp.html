<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Marcus Duarte webpage</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <!-- MathJax -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="posts.html">Posts</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="http://www.astro.iag.usp.br/~mvcduarte/">Astronomy</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="cv.html">CV</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="contact.html">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-image: url('img/rockets_nasa.jpg')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="site-heading">
              <h1>Welcome to my webpage!</h1>
              <span class="subheading">Machine Learning and Statistical Inferences</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">

            <h4 class="subheading">Artificial Neural Networks - Part 2: BackPropagation</h4>

            <p>
            <p, style="text-align:justify"> 
            In this serie of Neural Network posts, I demostrated in the <a href='ann_fp.html'>Part 1</a> how to implement the Artificial Neural Network (<a href='https://en.wikipedia.org/wiki/Artificial_neural_network'>ANN</a>) algorithm from scratch, more specifically the Foward Propagation. This second post of Neural Network will show the BackPropagation procedure, i.e., how to update the weights and bias described in the Part 1 to get closer and consequently achieve the best solution to reproduce the prediction/classification of your sample.   
            </p>

            <p, style="text-align:justify">
            Let us remember the formalism described in the Part 1 post. Following the same architecture shown in the previous post, the Foward Propagation can be defined in only one line, 
            </p>

            $$ a_3 = \sigma_2(\sum \underbrace{(\sigma_1(\sum X * W_1 + b_1))}_{a_2} * W_2 + b_2) $$
            
            </br>
            
            In Python syntax, the Foward Propagation is then written as, 

            <code style="color:green"><p>
            def </code><code style="color:red">foward_propagation</code><code style="color:black">(X, model):</br>
                &emsp;&emsp;"""</br>
                &emsp;&emsp; Foward Propagation</br>
                &emsp;&emsp;"""</br>
                </br>
                &emsp;&emsp;W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2'] </br>
                </br>
                &emsp;&emsp;z1 = X.dot(W1) + b1 </br>
                &emsp;&emsp;a2 = sigmoid(z1, deriv = False)</br>
                &emsp;&emsp;z2 = a2.dot(W2) + b2 </br>
                &emsp;&emsp;a3 = softmax(z2)</br>
                </br>
                &emsp;&emsp;return a3</br>
            </br>
            </code>
            being the activation function between the Input and Hidden Layer a sigmoid and a softmax between the Hidden Layer and the Output Layer. The values of weights and bias arrays are started in a randomic way and at the first iteraction or epoch, the prediction and classification is quite bad. 
            <p>

            In this way, our next step is to update the weights and bias arrays after each iteraction (Foward Propagation) in order to get closer to the desired goal. The Back Propagation calculates the difference between the obtained and target values at the output layer and calculates the gradient of weights and bias to update them. The Gradient Descent is employed here and it is based on the derivatives of the loss function \(E\) as a function of the weights and bias, i.e.,   
            $$ W_i \leftarrow W_i - \epsilon \frac{\partial E}{\partial W_i} \ \ \ \ \ ;\ \ \ \ \ b_i \leftarrow b_i - \epsilon \frac{\partial E}{\partial b_i}$$

            where \(\epsilon\) is known as learning rate. 
            </p>
            <p>
            The loss function can be defined in several ways but here we define it as an error function. Then, 

            $$ E = \frac{1}{2} \sum_i(a_{3,i} - Y_i)^2 $$

            where \(Y\) represents the target values of classification or regression and \(a_{3,i}\) is the i-th term of the Output Layer array. According to the equation of Gradient Descent above, our next step is to calculate the derivatives of loss function. Using the <a href='https://en.wikipedia.org/wiki/Chain_rule'>Chain Rule</a>, we can write the derivative of \(E\) regards \(W_2\) as

            $$ \frac{\partial E}{\partial W_2} = \underbrace{\frac{\partial E}{\partial a_3}}_{1} \underbrace{\frac{\partial a_3}{\partial z_2}}_{2} \underbrace{\frac{\partial z_2}{\partial W_2}}_{3} $$  
    
            and each derivative (1, 2 and 3) can be calculated in a separated way. The first term can calculated as the partial derivative of the sum, 

            $$ \frac{\partial E}{\partial a_{3}} = \frac{\partial}{\partial a_{3}} \left(\frac{1}{2}\sum_i(Y_i - a_{i,3})^2 \right) = (a_{3} - Y)$$

            The second term is basically the derivative of the activation term, 

            $$ \frac{\partial a_3}{\partial z_2} = \frac{\partial}{\partial z_2}\left( \sigma_2(z_2)\right) = \sigma_2'(z_2) $$

            where \(z_2 = a_2 * W_2 + b_2\). The last and third term can be written as, 

            $$ \frac{\partial z_2}{\partial W_2} = \frac{\partial}{\partial W_2}\left( \sum a_2 * W_2 + b_2 \right) = a_2 $$

            Note that all derivative above are arrays. Putting all terms together, the derivative of the error function regards \(W_2\) can be defined as, 

            $$ \frac{\partial E}{\partial W_2} = \underbrace{(a_3 - Y)\sigma'_2(z_2)}_{\delta_2} a_2 = a_2^T \delta_2 $$ 

            On the other hand, the derivative regards the bias factor \(b_2\) can be calculated using the same way as defined above for \(W_2\). Similarly, the derivative can be written as follows, 

            $$ \frac{\partial E}{\partial b_2} = \underbrace{\frac{\partial E}{\partial a_3}}_{1} \underbrace{\frac{\partial a_3}{\partial z_2}}_{2} \underbrace{\frac{\partial z_2}{\partial b_2}}_{3} $$  

            Note that the terms 1 and 2 were already calculated and only the third term needs to be calculated. This derivative is equals to unity, i.e., \( \frac{\partial a_3}{\partial b_2} = \frac{\partial}{\partial b_2} (b_2 + \sum a_2 * W_2)=1 \) so the equation above can be re-written as, 

            $$ \frac{\partial E}{\partial b_2} = (a_3 - Y) \sigma'_2(z_2)(1) = \delta_2 $$              
            <p>
            Similarly, we can calculate the derivatives regards the weights and bias terms between the Input and Hidden Layers, i.e., \(W_1\) and \(b_1\). However, these derivatives are a bit more complicated to be derived but still deductible,  

            $$ \frac{\partial E}{\partial W_1} = \frac{\partial E}{\partial a_2} \frac{\partial a_2}{\partial z_2} \frac{\partial z_2}{\partial W_1} = ... = X^T \delta_1 $$              

            where \(\delta_1 = \delta_2 W_2^T \sigma'_1(z_2) = \delta_2 W_2^T [a_2 * (1 - a_2)]\). Note that the derivative of the sigmoid function (activation function between the Input and Hidden Layer) is \(\sigma'(x) = \sigma(x) * (1 - \sigma(x)) = a * (1 - a)\), if you want the check its deduction, check this <a href= 'https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x'>link </a>. Writting the derivative of the error function regards the first bias term (\(b_1\)), 

            $$ \frac{\partial E}{\partial b_1} = \delta_1$$

            In practical terms, the Back Propagation algorithm written in Python can be expressed by the routine below.
            <code style="color:green"><p>
            def </code><code style="color:red">back_propagation</code><code style="color:black">(model_ann, a3, a2, model):</br>
                &emsp;&emsp;"""</br>
                &emsp;&emsp; Back Propagation</br>
                &emsp;&emsp;"""</br>
                </br>
                &emsp;&emsp;W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2'] </br>
                </br>
                &emsp;&emsp;# Backpropagating..</br>
                </br>
                &emsp;&emsp;# Define delta2 and delta1</br>
                &emsp;&emsp;delta2 = a3 </br>
                &emsp;&emsp;delta2[range(ann_model.n_train), ann_model.Y_train] -= 1 </br>
                &emsp;&emsp;delta1 = (delta2).dot(W2.T) * a2 * (1. - a2)</br>
                </br> 
                &emsp;&emsp;# Weights</br>
                </br> 
                &emsp;&emsp;dW2 = a2.T.dot(delta2)</br>
                &emsp;&emsp;dW1 = ann_model.X_train.T.dot(delta1)</br>
                </br> 
                &emsp;&emsp;# Bias</br>
                </br> 
                &emsp;&emsp;db2 = (delta2).sum(axis=0)</br>
                &emsp;&emsp;db1 = (delta1).sum(axis=0)</br>
                </br> 
                &emsp;&emsp;# Add regularization terms (not for b1 and b2)</br>
                </br> 
                &emsp;&emsp;dW2 += ann_model.reg_lambda * W2 </br>
                &emsp;&emsp;dW1 += ann_model.reg_lambda * W1</br>
                </br> 
                &emsp;&emsp;# Update parameter (gradient descent)</br>
                </br> 
                &emsp;&emsp;W1 += -ann_model.epsilon * dW1 </br>
                &emsp;&emsp;b1 += -ann_model.epsilon * db1 </br>
                &emsp;&emsp;W2 += -ann_model.epsilon * dW2 </br>
                &emsp;&emsp;b2 += -ann_model.epsilon * db2 </br>
                </br> 
                &emsp;&emsp;# Update parameters to the model </br>
                </br> 
                &emsp;&emsp;model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}</br> 
                </br> 
                &emsp;&emsp;return model</br>
            </br>
            </code>

            The regularization term (<code>reg_lambda</code>) is used in the routine above but not explictly explained here. This term is employed to avoid overfitting. It will be for another post in the future. The learning rate \(\epsilon\) defines the convergence speed, so if you define it large, your best solution will be only around the best solution but always oscillating around it. On the other hand, a relative too small \(\epsilon\) will need more iteractions (computational time) to converge to the best solution. What's the best value? It is a good exercise to start playing with the Neural Network code. Now, just consider <code>reg_lambda = 0</code>. 
            </p>
            <p>
            Ok, now all the routines are described and we can finally run our ANN. Then the working flow is the following: 
            </p>
            <ul>
            <li>
             Pass the training sample through the ANN, using the current weights and bias (Foward Propagation).
            </li>
            <li>
             Calculate the Gradient Descent using the Back Propagation formalism and update the weights \(W\) and bias \(b\) terms. 
            </li>
            <li>
             Once the weights and bias terms are updated, pass again the training sample through the Neural Network and restart the cycle.  
            </li>
            </ul>

            <p>
            Using the dataset <a href='http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html'>make_moons</a>, we run our Neural Network over 1,000 iteractions, calculating the Foward and Back Propagations each epoch. Figure 2 shows the parameter space, some elements or objects of the training sample and the shaded area indicates the classification and its border between the two classes (red and blue). At the top of the animation, we can see the number of iteractions of the Neural Network and the score (fraction of objects in the training sample correctly classified). As the number of iteractions increases, the weights and bias terms get closer to the solutions and consequently the objects are correctly classified. The borders defined by the Neural Network between the groups becomes more efficient and well defined. Figure 3 shows the fraction of objects wrongly classified as a function of the iteraction. It is clear that the updated weights and bias arrays get closer to the minimal, providing high fraction of correctly classified elements in the validation sample. It is important to mention that the higher the number of nodes in the hidden layer or number of hidden layers, the better is the classification as the training runs. Again, the overfitting starts having an important role in this high-complexity models. The example shown in Figure 2 uses 10 nodes in the hidden layers and presented a score of 97% at the end of the iteractions. For this example, we used the sigmoid at the Input-Hidden Layers and the softmax at the Hidden-Output Layers. Of course you can play with that and change the activation functions (e.g. tanh(x)) shows above, but the equations derivated above slightly change.  
            </p>

            <figure>
            <img class="img-fluid" src="graphs/animation_ann.gif" alt="">
            <figcaption><b>Figure 2</b>: The training sample in the feature space. Points in different colours represent distinct classes and the shaded areas shows the probability regions of classes. </figcaption>
            </figure>

            <figure>
            <img class="img-fluid" src="graphs/animation_ann_score.gif" alt="">
            <figcaption><b>Figure 3</b>: Fraction of wrongly classified objects in the validation sample as a function of the epochs or iteractions of the Neural Network. </figcaption>
            </figure>

            <p> Now you know how a Neural Network really works! If you are interested in playing a bit with this code, go to my Github and downloads the repository <a href='https://github.com/mvcduarte/Neural-Network'>GitHub - Neural Network from Scratch</a>. Remember to download the training/validation and test samples as well. Hereafter, you can also use other packages driven and optimized for Neural Network, such as <a href='https://keras.io/'>Keras</a>, <a href='https://www.tensorflow.org/'>Tensorflow</a> and <a href='http://deeplearning.net/software/theano/'> Theano</a>. There are also other techniques to update weights and bias terms when your trainins sample is huge! <a href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent'>Stochastic Gradient Descent</a> and <a href='https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/'>mini-batch Gradient Descent</a> represent alternatives to train your Neural Network. If you have comments/suggestions about this post, please send me an <a href="mailto:mvcduarte@gmail.com?Subject=blog_ML" target="_top">e-mail</a>.

            <p><b>Some references:</b></p>
              <p>
            <ul>
            <li><a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">Wild ML</a>
            </li>
            <li> 
              <a href="https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/">The Clever Machine's Blog"</a>
            </li>
            <li>
              <a href="https://beckernick.github.io/neural-network-scratch/">Nick Becker's blog</a>
            </li>

            <li>
              <a href="http://www.cristiandima.com/neural-networks-from-scratch-in-python/">Cristian Dima's Blog</a>
            </li>
            </ul>
           </div>
        </div>
      </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://www.facebook.com/marcus.duarte.92">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/mvcduarte">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Your Website 2018</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
